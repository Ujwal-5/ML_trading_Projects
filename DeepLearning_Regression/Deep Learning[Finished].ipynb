{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:05:20.422131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-06 15:05:21.541546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-06 15:05:21.541584: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-06 15:05:21.664351: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-06 15:05:24.657335: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 15:05:24.657477: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-06 15:05:24.657491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ujwal/ML_program/Trading_financial_market/Predicting_AUDUSD_Using_Major_Regression_Models')\n",
    "from ipynb.fs.defs.Data_preprocessing import sampling_multi_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps_back = 60\n",
    "n_steps_future=10\n",
    "\n",
    "train_X, train_Y, val_X, val_Y, test_X, test_Y,  = sampling_multi_target(n_steps_back, n_steps_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92464, 60)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92464, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82267, 0.82327, 0.82157, 0.82217, 0.82237, 0.82137, 0.82137,\n",
       "       0.82157, 0.81957, 0.82017, 0.82047, 0.82077, 0.82087, 0.82117,\n",
       "       0.82287, 0.82257, 0.82117, 0.82037, 0.82067, 0.82487, 0.82397,\n",
       "       0.82447, 0.82457, 0.82667, 0.82647, 0.82357, 0.82487, 0.82467,\n",
       "       0.82287, 0.82417, 0.82357, 0.82547, 0.82437, 0.82407, 0.82267,\n",
       "       0.82187, 0.82357, 0.82387, 0.82447, 0.82447, 0.82547, 0.82797,\n",
       "       0.82737, 0.82727, 0.82437, 0.82497, 0.82377, 0.82377, 0.82237,\n",
       "       0.81907, 0.82227, 0.82177, 0.82237, 0.82077, 0.81957, 0.81967,\n",
       "       0.82347, 0.82427, 0.82567, 0.82447])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82447, 0.82227, 0.82277, 0.82157, 0.82157, 0.82277, 0.82027,\n",
       "       0.82057, 0.82127, 0.82367])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82227, 0.82277, 0.82157, 0.82157, 0.82277, 0.82027, 0.82057,\n",
       "       0.82127, 0.82367, 0.82307])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 15:06:03.583045: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-06 15:06:03.583089: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-06 15:06:03.583115: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (linux-mint): /proc/driver/nvidia/version does not exist\n",
      "2022-12-06 15:06:03.583405: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               7808      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174,986\n",
      "Trainable params: 174,986\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "# import warnings \n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train_X.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(10, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0058 - mean_absolute_error: 0.0058\n",
      "Epoch 1: val_loss improved from inf to 0.00276, saving model to Weights-001--0.00276.hdf5\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0058 - mean_absolute_error: 0.0058 - val_loss: 0.0028 - val_mean_absolute_error: 0.0028\n",
      "Epoch 2/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0056 - mean_absolute_error: 0.0056\n",
      "Epoch 2: val_loss improved from 0.00276 to 0.00245, saving model to Weights-002--0.00245.hdf5\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0056 - mean_absolute_error: 0.0056 - val_loss: 0.0024 - val_mean_absolute_error: 0.0024\n",
      "Epoch 3/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0052 - mean_absolute_error: 0.0052\n",
      "Epoch 3: val_loss did not improve from 0.00245\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040\n",
      "Epoch 4/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0054 - mean_absolute_error: 0.0054\n",
      "Epoch 4: val_loss improved from 0.00245 to 0.00190, saving model to Weights-004--0.00190.hdf5\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0054 - mean_absolute_error: 0.0054 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 5/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0052 - mean_absolute_error: 0.0052\n",
      "Epoch 5: val_loss did not improve from 0.00190\n",
      "2312/2312 [==============================] - 11s 5ms/step - loss: 0.0052 - mean_absolute_error: 0.0052 - val_loss: 0.0026 - val_mean_absolute_error: 0.0026\n",
      "Epoch 6/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0050 - mean_absolute_error: 0.0050\n",
      "Epoch 6: val_loss did not improve from 0.00190\n",
      "2312/2312 [==============================] - 12s 5ms/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040\n",
      "Epoch 7/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0049 - mean_absolute_error: 0.0049\n",
      "Epoch 7: val_loss improved from 0.00190 to 0.00188, saving model to Weights-007--0.00188.hdf5\n",
      "2312/2312 [==============================] - 12s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 8/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0051 - mean_absolute_error: 0.0051\n",
      "Epoch 8: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 12s 5ms/step - loss: 0.0051 - mean_absolute_error: 0.0051 - val_loss: 0.0029 - val_mean_absolute_error: 0.0029\n",
      "Epoch 9/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0049 - mean_absolute_error: 0.0049\n",
      "Epoch 9: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 14s 6ms/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044\n",
      "Epoch 10/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0050 - mean_absolute_error: 0.0050\n",
      "Epoch 10: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 11s 5ms/step - loss: 0.0050 - mean_absolute_error: 0.0050 - val_loss: 0.0038 - val_mean_absolute_error: 0.0038\n",
      "Epoch 11/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0049 - mean_absolute_error: 0.0049\n",
      "Epoch 11: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 12s 5ms/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0037 - val_mean_absolute_error: 0.0037\n",
      "Epoch 12/100\n",
      "2300/2312 [============================>.] - ETA: 0s - loss: 0.0048 - mean_absolute_error: 0.0048\n",
      "Epoch 12: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 9s 4ms/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0020 - val_mean_absolute_error: 0.0020\n",
      "Epoch 13/100\n",
      "2305/2312 [============================>.] - ETA: 0s - loss: 0.0049 - mean_absolute_error: 0.0049\n",
      "Epoch 13: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0049 - mean_absolute_error: 0.0049 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047\n",
      "Epoch 14/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0048 - mean_absolute_error: 0.0048\n",
      "Epoch 14: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 8s 4ms/step - loss: 0.0048 - mean_absolute_error: 0.0048 - val_loss: 0.0046 - val_mean_absolute_error: 0.0046\n",
      "Epoch 15/100\n",
      "2299/2312 [============================>.] - ETA: 0s - loss: 0.0044 - mean_absolute_error: 0.0044\n",
      "Epoch 15: val_loss did not improve from 0.00188\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041\n",
      "Epoch 16/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0045 - mean_absolute_error: 0.0045\n",
      "Epoch 16: val_loss improved from 0.00188 to 0.00184, saving model to Weights-016--0.00184.hdf5\n",
      "2312/2312 [==============================] - 15s 7ms/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 17/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0044 - mean_absolute_error: 0.0044\n",
      "Epoch 17: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 18s 8ms/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0038 - val_mean_absolute_error: 0.0038\n",
      "Epoch 18/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0047 - mean_absolute_error: 0.0047\n",
      "Epoch 18: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 12s 5ms/step - loss: 0.0047 - mean_absolute_error: 0.0047 - val_loss: 0.0033 - val_mean_absolute_error: 0.0033\n",
      "Epoch 19/100\n",
      "2307/2312 [============================>.] - ETA: 0s - loss: 0.0044 - mean_absolute_error: 0.0044\n",
      "Epoch 19: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 9s 4ms/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0029 - val_mean_absolute_error: 0.0029\n",
      "Epoch 20/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0045 - mean_absolute_error: 0.0045\n",
      "Epoch 20: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0045 - mean_absolute_error: 0.0045 - val_loss: 0.0061 - val_mean_absolute_error: 0.0061\n",
      "Epoch 21/100\n",
      "2299/2312 [============================>.] - ETA: 0s - loss: 0.0042 - mean_absolute_error: 0.0042\n",
      "Epoch 21: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0030 - val_mean_absolute_error: 0.0030\n",
      "Epoch 22/100\n",
      "2300/2312 [============================>.] - ETA: 0s - loss: 0.0044 - mean_absolute_error: 0.0044\n",
      "Epoch 22: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 9s 4ms/step - loss: 0.0044 - mean_absolute_error: 0.0044 - val_loss: 0.0063 - val_mean_absolute_error: 0.0063\n",
      "Epoch 23/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0042 - mean_absolute_error: 0.0042\n",
      "Epoch 23: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0026 - val_mean_absolute_error: 0.0026\n",
      "Epoch 24/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0042 - mean_absolute_error: 0.0042\n",
      "Epoch 24: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 25/100\n",
      "2296/2312 [============================>.] - ETA: 0s - loss: 0.0042 - mean_absolute_error: 0.0042\n",
      "Epoch 25: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 26/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0041 - mean_absolute_error: 0.0041\n",
      "Epoch 26: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0031 - val_mean_absolute_error: 0.0031\n",
      "Epoch 27/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0041 - mean_absolute_error: 0.0041\n",
      "Epoch 27: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 8s 4ms/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0046 - val_mean_absolute_error: 0.0046\n",
      "Epoch 28/100\n",
      "2302/2312 [============================>.] - ETA: 0s - loss: 0.0042 - mean_absolute_error: 0.0042\n",
      "Epoch 28: val_loss did not improve from 0.00184\n",
      "2312/2312 [==============================] - 9s 4ms/step - loss: 0.0042 - mean_absolute_error: 0.0042 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 29/100\n",
      "2300/2312 [============================>.] - ETA: 0s - loss: 0.0041 - mean_absolute_error: 0.0041\n",
      "Epoch 29: val_loss improved from 0.00184 to 0.00162, saving model to Weights-029--0.00162.hdf5\n",
      "2312/2312 [==============================] - 9s 4ms/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0016 - val_mean_absolute_error: 0.0016\n",
      "Epoch 30/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 30: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 9s 4ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0023 - val_mean_absolute_error: 0.0023\n",
      "Epoch 31/100\n",
      "2307/2312 [============================>.] - ETA: 0s - loss: 0.0041 - mean_absolute_error: 0.0041\n",
      "Epoch 31: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0041 - mean_absolute_error: 0.0041 - val_loss: 0.0023 - val_mean_absolute_error: 0.0023\n",
      "Epoch 32/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0039\n",
      "Epoch 32: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 33/100\n",
      "2303/2312 [============================>.] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0039\n",
      "Epoch 33: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0020 - val_mean_absolute_error: 0.0020\n",
      "Epoch 34/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0040 - mean_absolute_error: 0.0040\n",
      "Epoch 34: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0040 - mean_absolute_error: 0.0040 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 35/100\n",
      "2304/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 35: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 11s 5ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 36/100\n",
      "2304/2312 [============================>.] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0039\n",
      "Epoch 36: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 8s 4ms/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0036 - val_mean_absolute_error: 0.0036\n",
      "Epoch 37/100\n",
      "2302/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 37: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 7s 3ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049\n",
      "Epoch 38/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0037\n",
      "Epoch 38: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0034 - val_mean_absolute_error: 0.0034\n",
      "Epoch 39/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 39: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040\n",
      "Epoch 40/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 40: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 7s 3ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0038 - val_mean_absolute_error: 0.0038\n",
      "Epoch 41/100\n",
      "2302/2312 [============================>.] - ETA: 0s - loss: 0.0039 - mean_absolute_error: 0.0039\n",
      "Epoch 41: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0039 - mean_absolute_error: 0.0039 - val_loss: 0.0023 - val_mean_absolute_error: 0.0023\n",
      "Epoch 42/100\n",
      "2299/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 42: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 12s 5ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0038 - val_mean_absolute_error: 0.0038\n",
      "Epoch 43/100\n",
      "2300/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 43: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 9s 4ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0052 - val_mean_absolute_error: 0.0052\n",
      "Epoch 44/100\n",
      "2300/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 44: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0020 - val_mean_absolute_error: 0.0020\n",
      "Epoch 45/100\n",
      "2297/2312 [============================>.] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0037\n",
      "Epoch 45: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 8s 3ms/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 46/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0037\n",
      "Epoch 46: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 47/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0037\n",
      "Epoch 47: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0036 - val_mean_absolute_error: 0.0036\n",
      "Epoch 48/100\n",
      "2307/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 48: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 10s 4ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0021 - val_mean_absolute_error: 0.0021\n",
      "Epoch 49/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0038 - mean_absolute_error: 0.0038\n",
      "Epoch 49: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 11s 5ms/step - loss: 0.0038 - mean_absolute_error: 0.0038 - val_loss: 0.0029 - val_mean_absolute_error: 0.0029\n",
      "Epoch 50/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0037 - mean_absolute_error: 0.0037\n",
      "Epoch 50: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 11s 5ms/step - loss: 0.0037 - mean_absolute_error: 0.0037 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 51/100\n",
      "2307/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 51: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 19s 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 52/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0036\n",
      "Epoch 52: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 17s 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0027 - val_mean_absolute_error: 0.0027\n",
      "Epoch 53/100\n",
      "2303/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 53: val_loss improved from 0.00162 to 0.00162, saving model to Weights-053--0.00162.hdf5\n",
      "2312/2312 [==============================] - 14s 6ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0016 - val_mean_absolute_error: 0.0016\n",
      "Epoch 54/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 54: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 22s 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0034 - val_mean_absolute_error: 0.0034\n",
      "Epoch 55/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0036\n",
      "Epoch 55: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 29s 13ms/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040\n",
      "Epoch 56/100\n",
      "2307/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 56: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 41s 18ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0030 - val_mean_absolute_error: 0.0030\n",
      "Epoch 57/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0036\n",
      "Epoch 57: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 36s 16ms/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 58/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 58: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 36s 16ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050\n",
      "Epoch 59/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 59: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 37s 16ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0038 - val_mean_absolute_error: 0.0038\n",
      "Epoch 60/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0036\n",
      "Epoch 60: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 30s 13ms/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 61/100\n",
      "2307/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 61: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 26s 11ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0025 - val_mean_absolute_error: 0.0025\n",
      "Epoch 62/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 62: val_loss did not improve from 0.00162\n",
      "2312/2312 [==============================] - 29s 12ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 63/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 63: val_loss improved from 0.00162 to 0.00160, saving model to Weights-063--0.00160.hdf5\n",
      "2312/2312 [==============================] - 34s 15ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0016 - val_mean_absolute_error: 0.0016\n",
      "Epoch 64/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0036 - mean_absolute_error: 0.0036\n",
      "Epoch 64: val_loss did not improve from 0.00160\n",
      "2312/2312 [==============================] - 33s 14ms/step - loss: 0.0036 - mean_absolute_error: 0.0036 - val_loss: 0.0035 - val_mean_absolute_error: 0.0035\n",
      "Epoch 65/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 65: val_loss did not improve from 0.00160\n",
      "2312/2312 [==============================] - 27s 12ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0024 - val_mean_absolute_error: 0.0024\n",
      "Epoch 66/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0035 - mean_absolute_error: 0.0035\n",
      "Epoch 66: val_loss did not improve from 0.00160\n",
      "2312/2312 [==============================] - 33s 14ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 67/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 67: val_loss did not improve from 0.00160\n",
      "2312/2312 [==============================] - 29s 13ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0026 - val_mean_absolute_error: 0.0026\n",
      "Epoch 68/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 68: val_loss improved from 0.00160 to 0.00154, saving model to Weights-068--0.00154.hdf5\n",
      "2312/2312 [==============================] - 34s 15ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0015 - val_mean_absolute_error: 0.0015\n",
      "Epoch 69/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 69: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 31s 13ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 70/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 70: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 29s 12ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 71/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 71: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 29s 13ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0029 - val_mean_absolute_error: 0.0029\n",
      "Epoch 72/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 72: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 30s 13ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0034 - val_mean_absolute_error: 0.0034\n",
      "Epoch 73/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 73: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 29s 13ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 74/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 74: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 31s 13ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 75/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 75: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 21s 9ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0022 - val_mean_absolute_error: 0.0022\n",
      "Epoch 76/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 76: val_loss did not improve from 0.00154\n",
      "2312/2312 [==============================] - 22s 10ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0022 - val_mean_absolute_error: 0.0022\n",
      "Epoch 77/100\n",
      "2305/2312 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 77: val_loss improved from 0.00154 to 0.00146, saving model to Weights-077--0.00146.hdf5\n",
      "2312/2312 [==============================] - 25s 11ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0015 - val_mean_absolute_error: 0.0015\n",
      "Epoch 78/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 78: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 23s 10ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0015 - val_mean_absolute_error: 0.0015\n",
      "Epoch 79/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 79: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 22s 10ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 80/100\n",
      "2304/2312 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 80: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 25s 11ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0027 - val_mean_absolute_error: 0.0027\n",
      "Epoch 81/100\n",
      "2307/2312 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 81: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 23s 10ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0026 - val_mean_absolute_error: 0.0026\n",
      "Epoch 82/100\n",
      "2305/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 82: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 20s 9ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0023 - val_mean_absolute_error: 0.0023\n",
      "Epoch 83/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 83: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 23s 10ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 84/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 84: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 22s 10ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040\n",
      "Epoch 85/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 85: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 22s 10ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0026 - val_mean_absolute_error: 0.0026\n",
      "Epoch 86/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 86: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 24s 10ms/step - loss: 0.0035 - mean_absolute_error: 0.0035 - val_loss: 0.0025 - val_mean_absolute_error: 0.0025\n",
      "Epoch 87/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 87: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 22s 10ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 88/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 88: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 24s 11ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0039 - val_mean_absolute_error: 0.0039\n",
      "Epoch 89/100\n",
      "2305/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 89: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 26s 11ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0019 - val_mean_absolute_error: 0.0019\n",
      "Epoch 90/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 90: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 25s 11ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0020 - val_mean_absolute_error: 0.0020\n",
      "Epoch 91/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0033\n",
      "Epoch 91: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 20s 9ms/step - loss: 0.0033 - mean_absolute_error: 0.0033 - val_loss: 0.0034 - val_mean_absolute_error: 0.0034\n",
      "Epoch 92/100\n",
      "2311/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 92: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 23s 10ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0015 - val_mean_absolute_error: 0.0015\n",
      "Epoch 93/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 93: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 20s 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 94/100\n",
      "2310/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 94: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 23s 10ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0035 - val_mean_absolute_error: 0.0035\n",
      "Epoch 95/100\n",
      "2308/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 95: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 20s 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 96/100\n",
      "2303/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 96: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 21s 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0029 - val_mean_absolute_error: 0.0029\n",
      "Epoch 97/100\n",
      "2312/2312 [==============================] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 97: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 20s 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0018 - val_mean_absolute_error: 0.0018\n",
      "Epoch 98/100\n",
      "2306/2312 [============================>.] - ETA: 0s - loss: 0.0032 - mean_absolute_error: 0.0032\n",
      "Epoch 98: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 19s 8ms/step - loss: 0.0032 - mean_absolute_error: 0.0032 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 99/100\n",
      "2309/2312 [============================>.] - ETA: 0s - loss: 0.0034 - mean_absolute_error: 0.0034\n",
      "Epoch 99: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 22s 10ms/step - loss: 0.0034 - mean_absolute_error: 0.0034 - val_loss: 0.0017 - val_mean_absolute_error: 0.0017\n",
      "Epoch 100/100\n",
      "2305/2312 [============================>.] - ETA: 0s - loss: 0.0031 - mean_absolute_error: 0.0031\n",
      "Epoch 100: val_loss did not improve from 0.00146\n",
      "2312/2312 [==============================] - 21s 9ms/step - loss: 0.0031 - mean_absolute_error: 0.0031 - val_loss: 0.0026 - val_mean_absolute_error: 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87c28529a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_model.fit(train_X, train_Y, epochs=100, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load wights file of the best model :\n",
    "wights_file = 'Weights-077--0.00146.hdf5' # choose the best checkpoint \n",
    "NN_model.load_weights(wights_file) # load it\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19760, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618/618 [==============================] - 3s 3ms/step\n",
      "618/618 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# test data\n",
    "preds_T = NN_model.predict(test_X)\n",
    "mae_T_t = mean_absolute_error(test_Y, preds_T)\n",
    "\n",
    "#val data\n",
    "preds_T_val = NN_model.predict(val_X)\n",
    "mae_T_v = mean_absolute_error(val_Y, preds_T_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE VALUE OF TEST DATA= 0.0015279386101732357 \n",
      "\n",
      "MAE VALUE OF VAL DATA= 0.0013110778463281433\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(\"MAE VALUE OF TEST DATA=\", mae_T_t, \"\\n\")\n",
    "print(\"MAE VALUE OF VAL DATA=\", mae_T_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred_labels_T_val')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(test_Y,preds_T)\n",
    "plt.xlabel('test_Y')\n",
    "plt.ylabel('pred_labels_T_val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66a19ea5f20bc52e6cb3c92a7c78abc463452fe56590fefcffb22eb4ed19e131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
